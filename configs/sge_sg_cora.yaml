defaults:
  - w2v_config

model:
  _target_: shallow_encoders.word2vec.model.SkipGram
  embedding_size: 16
  max_norm: null
  # vocab_size - unknown until dataset is loaded

datamodule:
  dataset_name: 'graph_cora'
  mode: 'sg'
  context_radius: 2
  max_length: 256
  min_word_frequency: 0
  additional_parameters:
    walks_per_node: 16
    walk_length: 10
    # method: deepwalk
    method: node2vec
    method_params:
      p: 1
      q: 2.0

  batch_size: 64
  num_workers: 8

train:
  experiment: 'SG_exp07_embedding16_node2vec_p1x0_q2x0'
  accelerator: 'gpu'  # (cpu, gpu)
  devices: '1'

  optimizer:
    _target_: torch.optim.Adam
    lr: 0.1

  max_epochs: 50

  scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 10
    gamma: 0.1

  loss:
    negative_samples: 1

analysis:
  checkpoint: 'last.ckpt'

  closest_pairs: true
  closest_pairs_per_word: 5
  visualize_embeddings: true
  visualize_embeddings_annotate: false
  visualize_embeddings_max_words: 10_000

  semantics_test: false

downstream:
  checkpoint: 'last.ckpt'

  # node classification
  node_classification: true
  node_classification_train_ratio: 1.0
  node_classification_n_experiments: 10
  node_classification_visualize: false

  # Link classification
  edge_classification: true
  edge_operator_name: 'hadamard'
  edge_classification_train_ratio: 0.5
  edge_classification_n_experiments: 10