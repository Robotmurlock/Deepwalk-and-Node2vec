defaults:
  - w2v_config

model:
  _target_: shallow_encoders.word2vec.model.SkipGram
  embedding_size: 12
  max_norm: null

datamodule:
  dataset_name: 'shakespeare'
  mode: 'sg'
  context_radius: 5
  max_length: 128
  min_word_frequency: 10
  lemmatize: true
  is_graph: false

  batch_size: 32
  num_workers: 8

train:
  experiment: 'SG_exp04_less_embeddings_no_norm'
  accelerator: 'gpu'  # (cpu, gpu)
  devices: '1'

  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-2

  max_epochs: 200

  scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 80
    gamma: 0.1

  loss:
    negative_samples: 3

analysis:
  checkpoint: 'last.ckpt'

  closest_pairs:
    enable: true
    pairs_per_word: 3
    max_words: 500

  visualize_embeddings:
    enable: true
    max_words: 250

  semantics_test:
    enable: true
